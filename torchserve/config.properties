# TorchServe Configuration

# Inference address and port
inference_address=http://0.0.0.0:8080

# Management address and port
management_address=http://0.0.0.0:8081

# Metrics address and port
metrics_address=http://0.0.0.0:8082

# Number of workers per model
# Увеличьте для лучшей производительности
default_workers_per_model=2

# Maximum response size
max_response_size=655350000

# Maximum request size (10MB)
max_request_size=655350000

# Job queue size
job_queue_size=100

# Enable model versioning
enable_envvars_config=true

# Model store location
model_store=/home/model-server/model-store

# Load models at startup
# Модель указана в CMD команде Dockerfile

# Logging
# Уровни: DEBUG, INFO, ERROR, WARN
default_log_level=INFO

# CORS settings (если нужно для веб-приложений)
# cors_allowed_origin=*
# cors_allowed_methods=GET, POST, PUT, OPTIONS
# cors_allowed_headers=*

